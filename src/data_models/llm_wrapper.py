from pydantic import BaseModel, Field, conint
from typing import Dict, Any, List, Optional, Literal

# --- 1. Core Wrapper Configuration ---
class WrapperConfig(BaseModel):
    """
    Configuration for the overall LLM wrapper, defining operational settings
    for managing local and provider LLMs.
    """
    # General settings
    log_level: str = Field("INFO", description="Logging level for the wrapper.")
    cache_enabled: bool = Field(True, description="Whether caching is enabled for responses.")
    # Add other global wrapper settings here

# --- 2. Local LLM Configuration (for tiny models) ---
class LocalLLMConfig(BaseModel):
    """
    Configuration for a specific local LLM, typically a tiny model.
    """
    model_name: str = Field(..., description="Identifier for the local LLM model.")
    model_path: Optional[str] = Field(None, description="Local path to the LLM model file (e.g., GGUF, safetensors).")
    model_type: Literal["llama.cpp", "transformers", "vllm_engine"] = Field(
        "llama.cpp", description="Type of local LLM integration."
    )
    # Resource allocation for local LLM (e.g., for llama.cpp, vLLM)
    n_gpu_layers: conint(ge=0) = Field(0, description="Number of GPU layers to offload (-1 for all).")
    n_ctx: conint(ge=1) = Field(2048, description="Text context window size.")
    n_batch: conint(ge=1) = Field(512, description="Prompt processing batch size.")
    # Add other local LLM specific settings like quantization, device mapping etc.

# --- 3. Model Parameters (for inference requests) ---
# These are general inference parameters that can apply to both local and provider LLMs.
class ModelParameters(BaseModel):
    """
    Parameters used for controlling LLM inference generation.
    """
    temperature: float = Field(0.7, ge=0.0, le=2.0, description="Controls randomness in generation. Higher values mean more random.")
    max_tokens: Optional[conint(ge=1)] = Field(1024, description="Maximum number of tokens to generate.")
    top_p: float = Field(1.0, ge=0.0, le=1.0, description="Nucleus sampling probability.")
    top_k: conint(ge=0) = Field(50, description="Top-k sampling for token selection.")
    stop_sequences: Optional[List[str]] = Field(None, description="Sequences where the model should stop generating tokens.")
    # Add other common inference parameters like frequency_penalty, presence_penalty etc.

# --- 4. Inference Request Structure ---
class InferenceRequest(BaseModel):
    """
    Represents a request to the LLM wrapper for generating text or processing a prompt.
    """
    prompt: str = Field(..., description="The input prompt string for the LLM.")
    model_id: Optional[str] = Field(None, description="Identifier of the specific LLM model to use (local or provider).")
    parameters: ModelParameters = Field(default_factory=ModelParameters, description="Inference parameters for the request.")
    # Contextual information (e.g., conversation history, tools to use)
    context: Dict[str, Any] = Field(default_factory=dict, description="Additional contextual data for the request.")
    stream: bool = Field(False, description="Whether to stream the response tokens.")
    # Add other request-specific fields like user_id, session_id etc.

# --- 5. Inference Response Structure ---
class InferenceResponse(BaseModel):
    """
    Represents the response from an LLM inference request.
    """
    generated_text: str = Field(..., description="The text generated by the LLM.")
    model_id: str = Field(..., description="Identifier of the LLM model that generated the response.")
    prompt_tokens: Optional[conint(ge=0)] = Field(None, description="Number of tokens in the input prompt.")
    completion_tokens: Optional[conint(ge=0)] = Field(None, description="Number of tokens generated in the response.")
    total_tokens: Optional[conint(ge=0)] = Field(None, description="Total tokens used (prompt + completion).")
    # Add other response metadata like finish_reason, latency, cached_response etc.
